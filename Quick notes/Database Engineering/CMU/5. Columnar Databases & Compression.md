- Ref: https://15445.courses.cs.cmu.edu/fall2022/notes/05-storage3.pdf
- Video: https://www.youtube.com/watch?v=q4W5r3GR0OU
### Types of Database workloads
1. OLTP - Fast operations that only read/update a small amount of data each time. e.g Amazon transactions.
2. OLAP - Complex queries that read a lot of data to compute aggregates. e.g Data Science, BI etc.
3. HTAP - OLTP + OLAP together on the same database instance

#### OLTP
- On-line Transaction Processing:
	→ Simple queries that read/update a small amount of data that is related to a single entity in the database.

#### OLAP
- On-line Analytical Processing:
	→ Complex queries that read large portions of the database spanning multiple entities.

#### Data Storage Models
- The DBMS can store tuples in different ways that are better for either OLTP or OLAP workloads.
1. **N-ary storage model (row-storage)**
	1. The DBMS stores all attributes for a single tuple contiguously in a page. Ideal for OLTP workloads where queries tend to operate only on an individual entity and insert heavy workloads.
	2. This is not very useful for useful for queries like below, yes we can use the (hostname, lastLogin) index called as covering index but that also have disadvantages associated.
	   ![[Pasted image 20230603155901.png]]
	3. Advantages:
	   - Fast inserts, updates, and deletes.
	   - Good for queries that need the entire tuple.
	4. Disadvantages:
	   - Not good for scanning large portions of the table and/or a subset of the attributes.

2. Decomposition Storage Model (DSM):
	1. The DBMS stores the values of a single attribute for all tuples contiguously in a page.
	   → Also known as a "column store"
	2. Ideal for OLAP workloads where read-only queries perform large scans over a subset of the table’s attributes.
	3. Tuple identification or stitching the records together. It has two design choices:
		   i. Fixed length offsets: Each value is the same length for an attribute.
		   ii. Embedded tuple ids: Each value is stored with its tuple id in a column.
	4.  Advantages:
		   - Reduces the amount wasted I/O because the DBMS only reads the data that it needs.
		   - Better query processing and data compression
	5. Disadvantages:
		- Slow for point queries, inserts, updates, and deletes because of tuple splitting/stitching.
	6. Examples: Amazon Redshift, MariaDB, SAP HANA, Hive, ClickHouse, presto, pinot

#### Observations
- I/O is the main bottleneck if the DBMS fetches data from disk during query execution.
- The DBMS can compress pages to increase the utility of the data moved per I/O operation.
- Key trade-off is speed vs. compression ratio:
	  - Compressing the database reduces DRAM requirements
	  - It may decrease CPU costs during query execution if the DBMS can operate on compressed data natively for queries and stuffs.
- Since datasets in real world generally have highly repeated values and well-defined structure which can be utilised to devise a compression strategy.
- Data sets tend to have high correlation between attributes of same tuple. Example: Zip Code to City, Order Date to Ship Date.

#### Database compression
- We always want to produce fixed length values so that we can use offset approach for stitching. Variable length data can be stored in separate pool with pointers to them and those can also be compressed as `gzip, zip` etc.
- Postpone decompression as long as possible during query execution. Also known as late materialization so that when passing along queries like String we can pass down hashes/compressed forms as far as possible and avoid transfer/comparison overheads.
- Must be a lossless scheme i.e guaranteed to get exact bytes back after decompression.
#### Compression granularity
